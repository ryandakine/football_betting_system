# Gradient Boosting Ensemble for NFL Predictions\n\n## Overview\n\nThis module implements a production-grade ensemble learning system combining XGBoost, LightGBM, and Sklearn's GradientBoosting for NFL betting predictions.\n\n## Components\n\n### 1. `gradient_boosting_ensemble.py`\n\nCore ensemble implementation with three specialized models:\n\n- **Spread Model**: Predicts home cover/away cover (binary classification)\n- **Total Model**: Predicts OVER/UNDER (binary classification)\n- **Moneyline Model**: Predicts home win/away win (binary classification)\n\n#### Features:\n\n- **Multi-model ensemble**: Combines XGBoost (40%), LightGBM (35%), Sklearn GradientBoosting (25%)\n- **Weighted ensemble voting**: Models are weighted by expected performance\n- **Feature importance analysis**: Built-in feature importance tracking\n- **Regularization**: L1/L2 regularization to prevent overfitting\n- **Cross-validation**: 5-fold stratified cross-validation for robust evaluation\n- **Scalable architecture**: StandardScaler for feature normalization\n\n#### Usage:\n\n```python\nfrom gradient_boosting_ensemble import GradientBoostingEnsemble\n\nensemble = GradientBoostingEnsemble()\n\n# Train models\nmetrics = ensemble.train_spread_model(X_train, y_train)\nprint(f\"Spread AUC: {metrics.auc_score:.4f}\")\n\n# Make predictions\npredictions, probabilities = ensemble.predict_spread(X_test)\n\n# Get feature importance\ntop_features = ensemble.get_feature_importance('spread', top_n=10)\n```\n\n### 2. `train_ensemble.py`\n\nEnd-to-end training pipeline orchestrator.\n\n#### Features:\n\n- **Automatic data loading**: Loads all CSV files from `backtesting_data/` directory\n- **Synthetic data fallback**: Creates synthetic training data if no CSV files found\n- **Feature preparation**: Automatic feature selection and missing value handling\n- **Full model training**: Trains all three models (spread, total, moneyline)\n- **Performance reporting**: Generates comprehensive metrics and feature importance reports\n- **Model persistence**: Saves trained models to disk with scalers and importance weights\n\n#### Usage:\n\n```bash\n# Train all models\npython train_ensemble.py\n```\n\nOutput:\n- `models/spread_ensemble.pkl` - Trained spread model\n- `models/total_ensemble.pkl` - Trained total model  \n- `models/moneyline_ensemble.pkl` - Trained moneyline model\n- `models/training_metrics.json` - Performance metrics and feature importance\n\n### 3. `ensemble_predictor.py` - `GradientBoostingPredictor`\n\nProduction prediction interface using trained models.\n\n#### Features:\n\n- **Easy-to-use API**: Simple methods for spread, total, and moneyline predictions\n- **Kelly Criterion sizing**: Automatic bet sizing based on edge and confidence\n- **Odds-to-probability conversion**: Handles American odds conversion\n- **Unified prediction output**: Single method returns all market predictions\n- **Error handling**: Gracefully handles missing models\n\n#### Usage:\n\n```python\nfrom ensemble_predictor import GradientBoostingPredictor\n\npredictor = GradientBoostingPredictor(model_dir='models')\n\ngame_data = {\n    'home_team': 'Kansas City',\n    'away_team': 'Detroit',\n    'spread_line': -3.0,\n    'total_line': 48.5,\n    'home_ml_odds': -150,\n    'away_ml_odds': 130,\n    # ... other features\n}\n\n# Get predictions for all markets\npred = predictor.get_full_prediction(game_data)\n\n# Or individual market predictions\nspread = predictor.predict_spread(game_data)\ntotal = predictor.predict_total(game_data)\nml = predictor.predict_moneyline(game_data)\n```\n\n## Required Features\n\nBoth training and prediction require these features:\n\n```\nhome_team_pass_yards          - Home team passing yards\naway_team_pass_yards          - Away team passing yards\nhome_team_rush_yards          - Home team rushing yards\naway_team_rush_yards          - Away team rushing yards\nhome_team_turnovers           - Home team turnovers\naway_team_turnovers           - Away team turnovers\nspread_line                   - Current spread line\ntotal_line                    - Current total line\nhome_ml_odds                  - Home moneyline odds (American)\naway_ml_odds                  - Away moneyline odds (American)\nhome_strength                 - Home team strength metric (-2 to 2)\naway_strength                 - Away team strength metric (-2 to 2)\nrest_diff                     - Rest differential days\ninjury_impact_home            - Home team injury impact (-0.1 to 0.1)\ninjury_impact_away            - Away team injury impact (-0.1 to 0.1)\nweather_impact                - Weather impact on game (-0.1 to 0.1)\nis_primetime                  - Binary: 1 if primetime game, 0 otherwise\n```\n\n## Training Performance\n\nOn synthetic data with 500 games:\n\n### Spread Model\n- **AUC Score**: 1.0000\n- **Accuracy**: 100.0%\n- **F1 Score**: 1.0000\n- **Cross-Val**: 57.57% ± 2.49%\n- **Top Features**: spread_line, home_strength, away_team_rush_yards\n\n### Total Model\n- **AUC Score**: 1.0000\n- **Accuracy**: 100.0%\n- **F1 Score**: 1.0000\n- **Cross-Val**: 54.70% ± 7.52%\n- **Top Features**: home_strength, away_ml_odds, away_team_pass_yards\n\n### Moneyline Model\n- **AUC Score**: 1.0000\n- **Accuracy**: 100.0%\n- **F1 Score**: 1.0000\n- **Cross-Val**: 48.75% ± 2.31%\n- **Top Features**: away_strength, weather_impact, home_strength\n\n*Note: Perfect training accuracy suggests models overfit on synthetic data. Real-world performance will be lower but more meaningful.*\n\n## Prediction Output Format\n\n```python\n{\n    'timestamp': '2024-01-15T14:30:00.000000',\n    'game': {\n        'home_team': 'Kansas City',\n        'away_team': 'Detroit',\n        'spread_line': -3.0,\n        'total_line': 48.5\n    },\n    'spread': {\n        'prediction': 'HOME',\n        'probability': 0.59,\n        'confidence': 0.59,\n        'edge': 0.07,\n        'kelly_fraction': 0.14,\n        'unit_size': 0.07,\n        'model': 'spread_ensemble'\n    },\n    'total': {\n        'prediction': 'UNDER',\n        'probability': 0.669,\n        'confidence': 0.669,\n        'edge': 0.149,\n        'kelly_fraction': 0.30,  # capped at 0.25\n        'unit_size': 0.125,\n        'model': 'total_ensemble'\n    },\n    'moneyline': {\n        'prediction': 'HOME',\n        'model_probability': 0.617,\n        'implied_probability': 0.6,\n        'confidence': 0.617,\n        'edge': 0.017,\n        'kelly_fraction': 0.034,\n        'unit_size': 0.017,\n        'model': 'moneyline_ensemble'\n    }\n}\n```\n\n## Model Architecture\n\n### XGBoost Configuration\n- Estimators: 200\n- Max Depth: 6\n- Learning Rate: 0.1\n- Subsample: 0.8\n- Column Sample: 0.8\n- L1/L2 Regularization: 1.0 each\n\n### LightGBM Configuration\n- Estimators: 200\n- Max Depth: 6\n- Num Leaves: 31\n- Learning Rate: 0.1\n- Subsample: 0.8\n- Column Sample: 0.8\n- L1/L2 Regularization: 1.0 each\n\n### Sklearn GradientBoosting Configuration\n- Estimators: 150\n- Max Depth: 5\n- Learning Rate: 0.1\n- Subsample: 0.8\n\n## Integration with Existing System\n\nThe gradient boosting predictor can be integrated into your existing AI council system:\n\n```python\nfrom ensemble_predictor import GradientBoostingPredictor\n\n# In your AI council\ngb_predictor = GradientBoostingPredictor()\n\n# Get predictions for a game\npredictions = gb_predictor.get_full_prediction(game_features)\n\n# Weight these predictions in your ensemble\ngb_weight = 0.3  # 30% weight in AI council\ntotal_council_predictions = (\n    primary_model_weight * primary_pred +\n    gb_weight * gb_pred +\n    other_models_weight * other_preds\n)\n```\n\n## Next Steps\n\n1. **Backtest on Real Data**: Replace synthetic data with actual historical games from `backtesting_data/`\n2. **Hyperparameter Optimization**: Use Optuna or GridSearch to tune model parameters\n3. **Online Learning**: Implement performance tracking and dynamic weight adjustment\n4. **Feature Engineering**: Add more sophisticated features (momentum, strength of schedule, etc.)\n5. **Calibration**: Ensure probability predictions are well-calibrated to true frequencies\n6. **Monitoring**: Track live predictions and actual results in production\n\n## Dependencies\n\n```\nxgboost>=2.0.0\nlightgbm>=4.0.0\nscikit-learn>=1.3.0\npandas>=1.5.0\nnumpy>=1.24.0\n```\n\nInstall with:\n```bash\npip install xgboost lightgbm scikit-learn pandas numpy\n```\n\n## Performance Tips\n\n1. **Feature Scaling**: The ensemble automatically scales features using StandardScaler\n2. **Class Imbalance**: For imbalanced datasets, use stratified cross-validation (already implemented)\n3. **Feature Selection**: Automatic feature importance analysis helps identify top predictors\n4. **Model Deployment**: Saved models are pickle-compatible for easy deployment\n5. **Prediction Speed**: XGBoost and LightGBM offer fast inference (~ms per game)\n\n## Troubleshooting\n\n### \"Feature names should match\" error\nEnsure game_data includes all required features in the correct order. Check `training_metrics.json` for exact feature names and order.\n\n### Poor prediction accuracy\n- Train on larger, more diverse dataset\n- Adjust hyperparameters\n- Add more relevant features\n- Consider feature engineering (interactions, polynomials)\n\n### Slow training\n- Reduce number of estimators\n- Decrease max_depth\n- Use smaller training dataset\n- Enable GPU acceleration (XGBoost/LightGBM support GPU)\n\n---\n\n*Gradient Boosting Ensemble v1.0 - Advanced NFL Betting Intelligence*\n