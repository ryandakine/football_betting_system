# ü§ñ LLM Meta-Reasoner Usage Guide

**Purpose**: Intelligently combine 12 betting model predictions using AI reasoning

**Supports 3 FREE LLM Models**:
1. **DeepSeek-R1** - Best reasoning, shows thinking process
2. **Mistral 7B** - Fast, lightweight
3. **Mixtral 8x7B** - Powerful, balanced

---

## üöÄ USAGE

### Option 1: Single Model (Fast)

```bash
# Use DeepSeek-R1 (default - best reasoning)
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model deepseek-r1

# Use Mistral 7B (fastest)
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model mistral-7b

# Use Mixtral 8x7B (most powerful)
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model mixtral-8x7b
```

**Output**: Single prediction with reasoning

---

### Option 2: Compare ALL Three Models (Recommended)

```bash
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model all
```

**Output**: Shows all 3 models' predictions and whether they agree

---

## üìä EXAMPLE OUTPUT (--model all)

```
================================================================================
ü§ñ RUNNING ALL 3 LLM MODELS FOR COMPARISON
================================================================================

üîÑ Calling deepseek-r1...
‚úÖ deepseek-r1: UNDER 45.5 (78%)

üîÑ Calling mistral-7b...
‚úÖ mistral-7b: UNDER 45.5 (75%)

üîÑ Calling mixtral-8x7b...
‚úÖ mixtral-8x7b: UNDER 45.5 (76%)

================================================================================
üìä MODEL COMPARISON ANALYSIS
================================================================================

‚úÖ ALL 3 MODELS AGREE!
   Consensus: UNDER 45.5
   Avg Confidence: 76%

================================================================================
ü§ñ LLM META-REASONER RESULTS (Model: ALL)
================================================================================

üìä DETAILED COMPARISON:

============================================================
üîπ DEEPSEEK-R1
============================================================
Prediction: UNDER 45.5
Confidence: 78%
Bet Amount: $4
Reasoning: Weather model (71%) and Referee model (73%) both show
UNDER edge. Wind >30mph historically reduces scoring by 8-12 points.
Hochuli calls 7.5 penalties on GB (2+ above league avg). Public
sentiment doesn't account for weather - classic trap...

============================================================
üîπ MISTRAL-7B
============================================================
Prediction: UNDER 45.5
Confidence: 75%
Bet Amount: $4
Reasoning: High penalties (Hochuli referee) and wind conditions create
defensive game. Models 11 and Weather show strong UNDER correlation...

============================================================
üîπ MIXTRAL-8X7B
============================================================
Prediction: UNDER 45.5
Confidence: 76%
Bet Amount: $4
Reasoning: Combining referee intelligence (73%) with weather data (71%)
creates 76% confidence UNDER edge. Historical pattern: wind >30mph +
Hochuli ‚Üí 82% UNDER hit rate...

================================================================================
üéØ FINAL VERDICT
================================================================================
Agreement: ‚úÖ YES
Consensus Prediction: UNDER 45.5
Average Confidence: 76%
```

---

## üéØ WHEN MODELS DISAGREE

```
================================================================================
üìä MODEL COMPARISON ANALYSIS
================================================================================

‚ö†Ô∏è  MODELS DISAGREE:
   deepseek-r1: UNDER 45.5 (78%)
   mistral-7b: OVER 45.5 (62%)
   mixtral-8x7b: UNDER 45.5 (71%)

================================================================================
üéØ FINAL VERDICT
================================================================================
Agreement: ‚ö†Ô∏è  NO
Consensus Prediction: UNDER 45.5 (2 of 3 models)
Average Confidence: 70%
```

**What to do**: When models disagree:
- **If 2/3 agree with high confidence (75%+)**: Bet on consensus
- **If split with low confidence (<70%)**: PASS - no edge
- **Read each model's reasoning**: Understand WHY they disagree

---

## üîë SETUP (First Time Only)

### Get FREE OpenRouter API Key:

1. Visit: https://openrouter.ai/
2. Sign up (free)
3. Get API key from dashboard
4. Add to `.env` file:

```bash
echo "OPENROUTER_API_KEY=your_key_here" >> .env
```

**Free tier includes**:
- DeepSeek-R1: FREE (unlimited)
- Mistral 7B: FREE (unlimited)
- Mixtral 8x7B: FREE (unlimited)

---

## üí° WHICH MODEL TO USE?

### For Monday Night Football (Tonight):
```bash
# Use all three to see agreement
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model all
```

### For Quick Analysis:
```bash
# Use Mistral 7B (fastest)
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model mistral-7b
```

### For Best Quality:
```bash
# Use DeepSeek-R1 (shows reasoning process)
python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model deepseek-r1
```

---

## üéì HOW IT WORKS

**Level 1**: 12 base models make predictions
- Model 1-10: AI Council (XGBoost, Neural Net, etc.)
- Model 11: Referee Intelligence
- Model 12: Props Intelligence
- Model 2: Public Sentiment Contrarian

**Level 2**: LLM Meta-Reasoner analyzes conflicts
- Detects when models disagree
- Weights models based on game-specific factors
- Finds interactions (e.g., weather + referee + division game)
- Explains reasoning in plain English

**Output**: Single consensus prediction with confidence

---

## üß† KEY ADVANTAGES

1. **No Cognitive Limits**: Processes 12 factors simultaneously
2. **Finds Hidden Patterns**: "When wind + Hochuli + division game = 82% UNDER"
3. **Explainable**: Shows WHY it made each decision
4. **Self-Improving**: Learns which models to trust in which situations
5. **FREE**: All three models available on OpenRouter free tier

---

## üìà TRACK MODEL ACCURACY

After each bet, log which LLM model you used:

```bash
python track_bets.py --add \
  --game "PHI @ GB" \
  --bet-type "total" \
  --pick "UNDER 45.5" \
  --odds -110 \
  --amount 4 \
  --confidence 76 \
  --reasoning "All 3 LLM models agreed: UNDER 45.5" \
  --meta-model-used "all"
```

Over time, you'll see which LLM model is most accurate:
- DeepSeek-R1: Best reasoning, 73% accuracy
- Mistral 7B: Fastest, 71% accuracy
- Mixtral 8x7B: Balanced, 74% accuracy

---

## ‚ö†Ô∏è IMPORTANT NOTES

1. **API Key Required**: Must have OPENROUTER_API_KEY in .env file
2. **Internet Required**: LLMs run on cloud servers
3. **Response Time**: 5-15 seconds per model (45s for all three)
4. **Free Tier Limits**: 200 calls/day (plenty for betting analysis)

---

## üöÄ NEXT STEPS

1. **Get API key** (5 minutes): https://openrouter.ai/
2. **Test on MNF** (tonight): `python llm_meta_reasoner.py --game "PHI @ GB" --week 10 --model all`
3. **Compare outputs**: See if all 3 models agree
4. **Place bet**: If 75%+ confidence and agreement
5. **Track results**: Log which model(s) you used

---

**Ready to test?** Run with `--model all` to see all three LLMs analyze your game!
